#First, need to copy ROI fastqs to my nearline directory, rename to match experimental names, rather than barcodes - needed to ask for these specially from Ellie, put in email next time to make sure we get them. 

mv bc1001.fastq.gz TI-MC3C-Dpn-t0Mit-4-30-T2.fastq.gz
mv bc1002.fastq.gz TI-MC3C-Dpn-t0Mit-4-39.fastq.gz
mv bc1003.fastq.gz TI-MC3C-Dpn-t2-4-39.fastq.gz
mv bc1008.fastq.gz TI-MC3C-Dpn-t4DMSO-4-39.fastq.gz
mv bc1009.fastq.gz TI-MC3C-Dpn-t4ICRF-4-39.fastq.gz
mv bc1010.fastq.gz TI-MC3C-Dpn-t8DMSO-4-39.fastq.gz
mv bc1011.fastq.gz TI-MC3C-Dpn-t8ICRF-4-39.fastq.gz

#From Filipe's pipeline:
#1. Filter long reads based on quality score - 80% of bases must be good quality (above 30). Since these are pacbio ROI - scores are very high, but I think care just about low scores, so should be ok? Might be able to rescue more reads by also trimming low quality regions from the end, but not doing that for now. I think the ROI might be already filtered for this? But I'm not sure. Or might be 0.75 threshold? Not doing anything for this step for now. 


#2. digest long-read fastqs at dpnII sites - doing this on the unfiltered ROI files for now, might want to repeat after filtering for quality. 
#Made a launch script for this

bash launch_digest_fastq_exp4_39.sh

#3. Running fastqc on digested and undigested fastqs. 
bash launch_fastqc_exp4_39.sh

#4 Using BWA to map digested fastqs to hg19. Edited this to have correct naming, and also to use span=1 on cluster, with only 8 cores, seems quite fast, I think 32 is overkill, since can't do all on one node so much slower. 
bash launch_bwa_mem_exp4_39.sh 

#Since sorting by name (-n) which is likely required for later steps to have fragments from each read together, can not index, so next steps remain slow...Don't need to separate out sorting and flagstat, were working fine all together with the mapping. 

#Start here 12/8/20

#5. Trying merge_mapped_reads_to_frags_stricter.py script - got this from Filipe's nearline directory, not from github. But I think it is required...
#Make sure to launch from interactive shell with conda MC3C-py38 env, updated script for pickle command - should be wb not w. 
#tested interactively and seems to work, but slow - running in long queue, overnight, hopefully will finish? - Yes! Would be better to speed this up in the future though...took many hours. 
#use launch script
bash launch_merge_mapped_reads_to_frags_exp4_39.sh

#Only one was able to finish, increasing time and trying again for rest


#start here 12/9/20 - only for t0 from 4-39 for now, will do rest when previous step finishes.
#6. Next need to onput the fragment mappings as txt (again, I got this from Filipe's nearline directory, not from github)
#Using a launch script again. From interactive shell, with conda activate MC3C-py38 env
#this outputs the fragments as walks
bash launch_output_frag_mappings_to_text_exp4_39_t0only.sh

#Same thing for rest of samples
bash launch_output_frag_mappings_to_text_exp4_39_restofsamples.sh

#7. Reduce_Frag_mappings - this turns walks into interactions, similar to pairs - probably with some editing can use to make cooler to check data?
#use a launch script
conda activate genomeinfodb
bash launch_reduce_frag_mappings_exp4_39_t0only.sh

#same for rest of samples
bash launch_reduce_frag_mappings_exp4_39_restofsamples.sh

#8. Trying interactions_to_usable_frame_stricter_exp4_39_t0only.R next, but will need to edit for my data, and also seems to be interactive, so will run in jupyter, genomeinfodb conda env. Need very large jupyter session. 
#This prepares the data for plotting. 

#9. Now have done this for the rest of the samples as well - and combined together into one direct_ints frame
#Ran into a problem with the R history loading all previously used variables - need to clear at start of each script to prevent mistakes. 
#This is probably why the spacings never looked right comparing real and permuted data, since it was not actually the permuted data...

201217_MC3C_interactions_to_usable_frame_stricter_4_30_AllSamples_NewColumns.ipynb
201211_MC3C_interactions_to_usable_frame_stricter_4_39_AllSamples.ipynb
201217_MC3C_CombiningR1andR2_interactions_to_plot.ipynb

#10. Ran permutations - had to increase number of cores to 64 in order to have enough memory for R1 + R2 reads. Only ran 10 so far. 
permutation_proof_of_principle_EMH_exp4.30and4.39.R
launch_permutations_exp4.30and4.39.sh

#11. Making permutations into frames for further analysis
interactions_to_usable_frame_for_permutations_EMH_R1R2_10seeds.R
#use launch script to start:
launch_permutations_to_frame_step2_exp4.30and4.39_10seeds.sh

#When done, try plotting
201230_permutations_to_frame_for_plotting_10seeds_R1R2.ipynb

#Want to make stranded pairs files for scaling plots, so re-running output_frag_mappings_to_text.py but adding strand info
bash launch_output_frag_mappings_to_text_stranded.sh

